{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# REGRESIÓN LINEAL MÚLTIPLE\n",
   "id": "dbe7829c7bb4b662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Crea el fichero u02_p03_a1_reglin_multi_<tus_iniciales>.py. Utiliza pandas y carga los datos del\n",
    "fichero \"50_startups.csv\". Utilizaremos como predictoras las columnas \"R&D Spend\", \"Marqueting\n",
    "Spend\" y \"State\" y como target usaremos \"Profit\" (beneficios). Imprime los 5 primeros ejemplos de las\n",
    "predictoras: <br><br>\n",
    "Para realizar regresión lineal no podemos usar una columna categórica como \"State\". Codifica sus\n",
    "valores usando el método one-hot-encoder de manera que aparecerán 3 nuevas columnas que\n",
    "corresponden a cada uno de los valores que puede tener la columna original (‘New York’, ‘California’,\n",
    "‘Florida’) y sus valores estarán a 1 cuando sea ese valor y a 0 cuando no. En vez de hacerlo a mano,\n",
    "vamos a utilizar los objetos sklearn.compose.ColumnTransformer y\n",
    "sklearn.preprocessing.OneHotEncoder. Busca información de como hacerlo. Tras realizarlo imprime\n",
    "las primeras 5 filas de los datos transformados.<br><br>\n",
    "Como todo es un proceso aleatorio, para poder obtener resultados comparables utilizamos la semilla\n",
    "\"123\" en todos los procesos. Divide los datos en train y test dejando el 80% para entrenamiento.\n",
    "Imprime los 5 primeros ejemplos de X_train, y_train, X_test e y_test:"
   ],
   "id": "94755f9e1666d3d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Imporación de librerías",
   "id": "a7ba6ea949ccde1e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.351188Z",
     "start_time": "2025-02-13T19:26:40.343603Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.428358Z",
     "start_time": "2025-02-13T19:26:40.411160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"recursos/U02_P03/50_Startups.csv\")\n",
    "# Definir las variables predictoras (X) y la variable objetivo (y)\n",
    "X = df[['R&D Spend', 'Marketing Spend', 'State']]\n",
    "y = df['Profit']"
   ],
   "id": "7a72d3b34341fab1",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.511482Z",
     "start_time": "2025-02-13T19:26:40.492700Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "28b1e0de2b21c403",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.581324Z",
     "start_time": "2025-02-13T19:26:40.570695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Primeras 5 filas de las variables predictoras:\")\n",
    "print(X.head())"
   ],
   "id": "6417c2e5a900c6aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 5 filas de las variables predictoras:\n",
      "   R&D Spend  Marketing Spend       State\n",
      "0  165349.20        471784.10    New York\n",
      "1  162597.70        443898.53  California\n",
      "2  153441.51        407934.54     Florida\n",
      "3  144372.41        383199.62    New York\n",
      "4  142107.34        366168.42     Florida\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.671724Z",
     "start_time": "2025-02-13T19:26:40.663408Z"
    }
   },
   "cell_type": "code",
   "source": "np.random.seed(123)",
   "id": "3f9887b2b752ff98",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.800167Z",
     "start_time": "2025-02-13T19:26:40.783718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definir las columnas numéricas y categóricas\n",
    "columnas_numericas = [\"R&D Spend\", \"Marketing Spend\"]\n",
    "columnas_categoricas = [\"State\"]\n",
    "\n",
    "# Crear un ColumnTransformer para aplicar OneHotEncoder a la columna \"State\"\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"onehot\", OneHotEncoder(), columnas_categoricas)  # Aplicar OneHotEncoder a \"State\"\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Mantener las demás columnas sin cambios\n",
    ")\n",
    "\n",
    "# Aplicar la transformación\n",
    "X_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Convertir a DataFrame para ver mejor los resultados\n",
    "nombres_columnas = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Renombramos las columnas para eliminar los prefijos 'remainder__' y '__'\n",
    "nombres_columnas = [col.replace(\"remainder__\", \"\") for col in nombres_columnas]\n",
    "df_transformado = pd.DataFrame(X_transformed, columns=nombres_columnas)\n",
    "\n",
    "# Imprimir las primeras 5 filas\n",
    "print(\"Primeras 5 filas de los datos transformados:\")\n",
    "print(df_transformado.head())"
   ],
   "id": "3125ee6fd61ef0d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 5 filas de los datos transformados:\n",
      "   onehot__State_California  onehot__State_Florida  onehot__State_New York  \\\n",
      "0                       0.0                    0.0                     1.0   \n",
      "1                       1.0                    0.0                     0.0   \n",
      "2                       0.0                    1.0                     0.0   \n",
      "3                       0.0                    0.0                     1.0   \n",
      "4                       0.0                    1.0                     0.0   \n",
      "\n",
      "   R&D Spend  Administration  Marketing Spend     Profit  \n",
      "0  165349.20       136897.80        471784.10  192261.83  \n",
      "1  162597.70       151377.59        443898.53  191792.06  \n",
      "2  153441.51       101145.55        407934.54  191050.39  \n",
      "3  144372.41       118671.85        383199.62  182901.99  \n",
      "4  142107.34        91391.77        366168.42  166187.94  \n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:40.892992Z",
     "start_time": "2025-02-13T19:26:40.871659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definir las variables predictoras (X) y la variable objetivo (y)\n",
    "X = df_transformado.drop('Profit', axis=1)  # La columna 'Profit' es la variable objetivo y la eliminamos de X\n",
    "y = df['Profit']  # 'Profit' es la variable objetivo original, sin transformación\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Verificar las primeras filas de X_train para confirmar que los datos están bien\n",
    "print(\"\\nPrimeros 5 ejemplos de X_train:\")\n",
    "print(X_train.head())"
   ],
   "id": "81db2b408709bb9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeros 5 ejemplos de X_train:\n",
      "    onehot__State_California  onehot__State_Florida  onehot__State_New York  \\\n",
      "21                       0.0                    0.0                     1.0   \n",
      "47                       1.0                    0.0                     0.0   \n",
      "11                       1.0                    0.0                     0.0   \n",
      "41                       0.0                    1.0                     0.0   \n",
      "5                        0.0                    0.0                     1.0   \n",
      "\n",
      "    R&D Spend  Administration  Marketing Spend  \n",
      "21   78389.47       153773.43        299737.29  \n",
      "47       0.00       135426.92             0.00  \n",
      "11  100671.96        91790.61        249744.55  \n",
      "41   27892.92        84710.77        164470.71  \n",
      "5   131876.90        99814.71        362861.36  \n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T19:26:41.374931Z",
     "start_time": "2025-02-13T19:26:41.011582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear un pipeline con el preprocesador y el modelo de regresión lineal\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y obtener el score de entrenamiento y prueba\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)"
   ],
   "id": "cf059fd3747077c",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'State'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:361\u001B[0m, in \u001B[0;36m_get_column_indices\u001B[1;34m(X, key)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[1;32m--> 361\u001B[0m     col_idx \u001B[38;5;241m=\u001B[39m \u001B[43mall_columns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col_idx, numbers\u001B[38;5;241m.\u001B[39mIntegral):\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'State'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[124], line 8\u001B[0m\n\u001B[0;32m      2\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline(steps\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m      3\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessor\u001B[39m\u001B[38;5;124m'\u001B[39m, preprocessor),\n\u001B[0;32m      4\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregressor\u001B[39m\u001B[38;5;124m'\u001B[39m, LinearRegression())\n\u001B[0;32m      5\u001B[0m ])\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Entrenar el modelo con los datos de entrenamiento\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Hacer predicciones y obtener el score de entrenamiento y prueba\u001B[39;00m\n\u001B[0;32m     11\u001B[0m train_score \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mscore(X_train, y_train)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\pipeline.py:469\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    426\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model.\u001B[39;00m\n\u001B[0;32m    427\u001B[0m \n\u001B[0;32m    428\u001B[0m \u001B[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;124;03m    Pipeline with fitted steps.\u001B[39;00m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    468\u001B[0m routed_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_method_params(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, props\u001B[38;5;241m=\u001B[39mparams)\n\u001B[1;32m--> 469\u001B[0m Xt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m    471\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\pipeline.py:406\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[1;34m(self, X, y, routed_params)\u001B[0m\n\u001B[0;32m    404\u001B[0m     cloned_transformer \u001B[38;5;241m=\u001B[39m clone(transformer)\n\u001B[0;32m    405\u001B[0m \u001B[38;5;66;03m# Fit or load from cache the current transformer\u001B[39;00m\n\u001B[1;32m--> 406\u001B[0m X, fitted_transformer \u001B[38;5;241m=\u001B[39m \u001B[43mfit_transform_one_cached\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcloned_transformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    409\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;66;03m# Replace the transformer of the step with the fitted\u001B[39;00m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;66;03m# transformer. This is necessary when loading the transformer\u001B[39;00m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;66;03m# from the cache.\u001B[39;00m\n\u001B[0;32m    418\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[step_idx] \u001B[38;5;241m=\u001B[39m (name, fitted_transformer)\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\joblib\\memory.py:312\u001B[0m, in \u001B[0;36mNotMemorizedFunc.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\pipeline.py:1310\u001B[0m, in \u001B[0;36m_fit_transform_one\u001B[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001B[0m\n\u001B[0;32m   1308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[0;32m   1309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1310\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit_transform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1311\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1312\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[0;32m   1313\u001B[0m             X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[0;32m   1314\u001B[0m         )\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 316\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    318\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    319\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    320\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    321\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    322\u001B[0m         )\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:968\u001B[0m, in \u001B[0;36mColumnTransformer.fit_transform\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    965\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_transformers()\n\u001B[0;32m    966\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(X)\n\u001B[1;32m--> 968\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_column_callables\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_remainder(X)\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _routing_enabled():\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:536\u001B[0m, in \u001B[0;36mColumnTransformer._validate_column_callables\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    534\u001B[0m         columns \u001B[38;5;241m=\u001B[39m columns(X)\n\u001B[0;32m    535\u001B[0m     all_columns\u001B[38;5;241m.\u001B[39mappend(columns)\n\u001B[1;32m--> 536\u001B[0m     transformer_to_input_indices[name] \u001B[38;5;241m=\u001B[39m \u001B[43m_get_column_indices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_columns \u001B[38;5;241m=\u001B[39m all_columns\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transformer_to_input_indices \u001B[38;5;241m=\u001B[39m transformer_to_input_indices\n",
      "File \u001B[1;32m~\\Desktop\\MachineLearning\\saa\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:369\u001B[0m, in \u001B[0;36m_get_column_indices\u001B[1;34m(X, key)\u001B[0m\n\u001B[0;32m    366\u001B[0m         column_indices\u001B[38;5;241m.\u001B[39mappend(col_idx)\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 369\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA given column is not a column of the dataframe\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m column_indices\n",
      "\u001B[1;31mValueError\u001B[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\nScore en datos de entrenamiento: {train_score}\")\n",
    "print(f\"Score en datos de prueba: {test_score}\")"
   ],
   "id": "bb7bab58cdc607e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Funciones para calcular SSE, SST y SSR\n",
    "def calculate_SSE(y_true, y_pred):\n",
    "    \"\"\" Calcula el SSE (Sum of Squared Errors) \"\"\"\n",
    "    return np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "def calculate_SST(y_true):\n",
    "    \"\"\" Calcula el SST (Total Sum of Squares) \"\"\"\n",
    "    return np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "\n",
    "def calculate_SSR(SST, SSE):\n",
    "    \"\"\" Calcula el SSR (Sum of Squares for Regression) \"\"\"\n",
    "    return SST - SSE\n",
    "\n",
    "# Calcular las métricas para los datos de entrenamiento\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "SSE_train = calculate_SSE(y_train, y_train_pred)\n",
    "SST_train = calculate_SST(y_train)\n",
    "SSR_train = calculate_SSR(SST_train, SSE_train)\n",
    "\n",
    "# Calcular R² para entrenamiento\n",
    "R2_train = 1 - SSE_train / SST_train\n",
    "\n",
    "# Calcular R² ajustado para entrenamiento\n",
    "n_train = len(y_train)\n",
    "k_train = X_train.shape[1]\n",
    "R2_adj_train = 1 - (SSE_train / (n_train - k_train - 1)) / (SST_train / (n_train - 1))"
   ],
   "id": "9db529170f857f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Imprimir las métricas para los datos de entrenamiento\n",
    "print(f\"Entrenamiento:\")\n",
    "print(f\"SSE: {SSE_train}\")\n",
    "print(f\"SST: {SST_train}\")\n",
    "print(f\"SSR: {SSR_train}\")\n",
    "print(f\"R²: {R2_train}\")\n",
    "print(f\"R² ajustado: {R2_adj_train}\")"
   ],
   "id": "9fba953e588c42c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calcular las métricas para los datos de prueba\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "SSE_test = calculate_SSE(y_test, y_test_pred)\n",
    "SST_test = calculate_SST(y_test)\n",
    "SSR_test = calculate_SSR(SST_test, SSE_test)\n",
    "\n",
    "# Calcular R² para prueba\n",
    "R2_test = 1 - SSE_test / SST_test\n",
    "\n",
    "# Calcular R² ajustado para prueba\n",
    "n_test = len(y_test)\n",
    "k_test = X_test.shape[1]\n",
    "R2_adj_test = 1 - (SSE_test / (n_test - k_test - 1)) / (SST_test / (n_test - 1))"
   ],
   "id": "966b0d80acd895e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Imprimir las métricas para los datos de prueba\n",
    "print(f\"\\nPrueba:\")\n",
    "print(f\"SSE: {SSE_test}\")\n",
    "print(f\"SST: {SST_test}\")\n",
    "print(f\"SSR: {SSR_test}\")\n",
    "print(f\"R²: {R2_test}\")\n",
    "print(f\"R² ajustado: {R2_adj_test}\")"
   ],
   "id": "6cd6626a564bfa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Incluir \"Administration\" como una nueva característica\n",
    "columnas_nuevas = [\"R&D Spend\", \"Administration\", \"Marketing Spend\", \"State\"]\n",
    "\n",
    "# Crear el nuevo DataFrame con las características\n",
    "X_nuevas = df[columnas_nuevas]\n",
    "y_nuevos = df['Profit']\n",
    "\n",
    "# Dividir nuevamente los datos en entrenamiento y prueba\n",
    "X_train_nuevas, X_test_nuevas, y_train_nuevos, y_test_nuevos = train_test_split(X_nuevas, y_nuevos, test_size=0.2, random_state=123)\n",
    "\n",
    "# Crear un nuevo pipeline para el modelo con las nuevas características\n",
    "preprocessor_nuevas = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"onehot\", OneHotEncoder(), [\"State\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Crear el pipeline\n",
    "pipeline_nuevas = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_nuevas),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con las nuevas características\n",
    "pipeline_nuevas.fit(X_train_nuevas, y_train_nuevos)"
   ],
   "id": "acc20c8d8bf4bb9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calcular y mostrar las métricas para el nuevo modelo\n",
    "y_train_pred_nuevas = pipeline_nuevas.predict(X_train_nuevas)\n",
    "SSE_train_nuevas = calculate_SSE(y_train_nuevos, y_train_pred_nuevas)\n",
    "SST_train_nuevas = calculate_SST(y_train_nuevos)\n",
    "SSR_train_nuevas = calculate_SSR(SST_train_nuevas, SSE_train_nuevas)\n",
    "R2_train_nuevas = 1 - SSE_train_nuevas / SST_train_nuevas\n",
    "R2_adj_train_nuevas = 1 - (SSE_train_nuevas / (len(y_train_nuevos) - X_train_nuevas.shape[1] - 1)) / (SST_train_nuevas / (len(y_train_nuevos) - 1))"
   ],
   "id": "dbc4576bd137335f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\nNuevo modelo de entrenamiento:\")\n",
    "print(f\"SSE: {SSE_train_nuevas}\")\n",
    "print(f\"SST: {SST_train_nuevas}\")\n",
    "print(f\"SSR: {SSR_train_nuevas}\")\n",
    "print(f\"R²: {R2_train_nuevas}\")\n",
    "print(f\"R² ajustado: {R2_adj_train_nuevas}\")"
   ],
   "id": "a244d096276be5d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Hacer lo mismo para los datos de prueba\n",
    "y_test_pred_nuevas = pipeline_nuevas.predict(X_test_nuevas)\n",
    "SSE_test_nuevas = calculate_SSE(y_test_nuevos, y_test_pred_nuevas)\n",
    "SST_test_nuevas = calculate_SST(y_test_nuevos)\n",
    "SSR_test_nuevas = calculate_SSR(SST_test_nuevas, SSE_test_nuevas)\n",
    "R2_test_nuevas = 1 - SSE_test_nuevas / SST_test_nuevas\n",
    "R2_adj_test_nuevas = 1 - (SSE_test_nuevas / (len(y_test_nuevos) - X_test_nuevas.shape[1] - 1)) / (SST_test_nuevas / (len(y_test_nuevos) - 1))"
   ],
   "id": "86603e3974827657"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\nNuevo modelo de prueba:\")\n",
    "print(f\"SSE: {SSE_test_nuevas}\")\n",
    "print(f\"SST: {SST_test_nuevas}\")\n",
    "print(f\"SSR: {SSR_test_nuevas}\")\n",
    "print(f\"R²: {R2_test_nuevas}\")\n",
    "print(f\"R² ajustado: {R2_adj_test_nuevas}\")"
   ],
   "id": "b2b93e35fd398b0b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
