{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üìåACTIVIDAD 2: UN CASO M√ÅS REAL.",
   "id": "33b81e401e708bdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Crea el fichero u02_p03_a2_reglin_<tus_iniciales>.py. Vamos a trabajar con los datos del fichero\n",
    "\"seguros_de_coches.csv\" que utiliza una compa√±√≠a aseguradora. <br><br>\n",
    "COMPRENDER LOS DATOS DEL DATASET<br><br>\n",
    "Hay 3 tipos de caracter√≠sticas:<br><br>\n",
    "(a) La especificaci√≥n de un autom√≥vil en t√©rminos de diferentes caracter√≠sticas.<br><br>\n",
    "(b) Su calificaci√≥n de riesgo de seguro asignada. Es un indicador del grado en que el\n",
    "autom√≥vil es m√°s problem√°tico de lo que indica su precio. Inicialmente, a los autom√≥viles se\n",
    "les asigna un s√≠mbolo de factor de riesgo asociado con su precio. Luego, si es m√°s arriesgado (o\n",
    "menos), este s√≠mbolo se ajusta movi√©ndolo hacia arriba (o hacia abajo) en la escala. Los\n",
    "profesionales llaman a este proceso ‚Äúsymboling‚Äù. Un valor de 3 indica que el autom√≥vil es\n",
    "riesgoso y -3 que probablemente sea bastante seguro.<br><br>\n",
    "(c) El pago medio relativo por p√©rdida por a√±o de veh√≠culo asegurado. Este valor est√°\n",
    "normalizado para todos los autom√≥viles dentro de una clasificaci√≥n de tama√±o particular (two\u0002door small, station wagons, sports/speciality, etc‚Ä¶) y representa la p√©rdida promedio por\n",
    "autom√≥vil por a√±o.<br><br>\n",
    "Hay que tener en cuenta que los valores ausentes est√°n definidos con un ‚Äú?‚Äù. Completa los siguientes\n",
    "pasos en el fichero Python comenzando por:<br><br>\n",
    "a) Cargar los datos en un Dataframe, (adapta la ruta al c√≥digo de la siguiente figura). <br><br>\n",
    "b) Mostrar los 5 primeros ejemplos por pantalla (completa y obt√©n mismos resultados).<br><br>\n",
    "c) Muestra un resumen de las columnas del dataset (completa y obt√©n mismos resultados). <br><br>\n"
   ],
   "id": "8cb01d2c758e72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "columnas = ['symboling', 'perdidas_normal', 'marca', 'combustible', 'inyecci√≥n',\n",
    "            'puertas', 'chasis', 'traccion', 'lugar_de_motor', 'potencia_base',\n",
    "            'longitud', 'anchura', 'altura', 'peso', 'tipo_motor',\n",
    "            'cilindros', 'tama√±o_motor', 'sistema', 'calibre', 'ataque',\n",
    "            'ratio_compresion', 'potencia_cv', 'max_rpm', 'consumo_carretera', 'consumo_ciudad',\n",
    "            'precio']\n",
    "\n",
    "autos = pd.read_csv(\"seguros_de_coches.csv\", header=None, names=columnas, na_values='?', delimiter=\",\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DATOS AUSENTES Y CATEG√ìRICOS<br><br>\n",
    "Los datos se han cargado remplazando los s√≠mbolos ‚Äú?‚Äù como valores ausentes NaN. como estaba\n",
    "definido en la informaci√≥n del dataset. Primero sumarizamos los datos ausentes por caracter√≠stica.\n",
    "Luego nos podemos fijar en el listado que nos ha devuelto el m√©todo autos.info() y todas las que no\n",
    "tengan un tipo num√©rico (intX y floatX) ser√°n categ√≥ricas de una u otra forma"
   ],
   "id": "c859bfaf4c22a560"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Valores ausentes y columnas categ√≥ricas\n",
    "print(\"========== Valores ausentes:\\n\", autos.isna().sum())\n",
    "\n",
    "cols_categoricas = [\"marca\", \"sistema\"]  # Completa el array!!!\n",
    "\n",
    "autos[cols_categoricas] = autos[cols_categoricas].astype(\"category\")\n",
    "\n",
    "autos[\"puertas\"] = pd.Categorical(autos[\"puertas\"], categories=[\"two\", \"four\"], ordered=True)\n",
    "\n",
    "autos[\"cilindros\"] = pd.Categorical(autos[\"cilindros\"],\n",
    "                                    categories=[\"two\", \"three\", \"four\", \"five\", \"six\", \"eight\", \"twelve\"],\n",
    "                                    ordered=True)\n",
    "\n",
    "print(\"========== Cambiamos a tipo category:\\n\", autos.info())"
   ],
   "id": "ee0616a12141da90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para corregirlas podemos crearnos un array con sus nombres.\n",
    "Completa el array cols_categoricas de la figura con el resto de\n",
    "caracter√≠sticas categ√≥ricas (ahora solamente aparecen la primera y\n",
    "la √∫ltima). Usamos este array para cambiarles el tipo de dato. Y a\n",
    "dos caracter√≠sticas ordinales le indicamos que lo son. Por √∫ltimo\n",
    "mostramos informaci√≥n del dataframe. Deber√≠as obtener algo\n",
    "similar a esto como salida.<br><br>\n",
    "Descripci√≥n estad√≠stica<br><br>\n",
    "Se debe hacer un an√°lisis de cada una de las variables y describir\n",
    "sus propiedades. Realizar el an√°lisis univariado es muy\n",
    "importante para entender el comportamiento de cada una de las\n",
    "variables y poder detectar posibles problemas en los datos. Nunca\n",
    "hay que saltarse este paso. Por ejemplo vas a contar cuantas veces\n",
    "aparece cada valor de las columnas categ√≥ricas. Aquellas\n",
    "columnas que tengan valores que solamente aparezcan una vez,\n",
    "las a√±ades al array cols_categoricas_escasas."
   ],
   "id": "4c5dc51e7054cbda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cols_categoricas_escasas = []  # Completa el array\n",
    "\n",
    "for col in cols_categoricas:\n",
    "    print(autos[col].value_counts())\n",
    "    print(\"\")"
   ],
   "id": "630f71b131cc408c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Algunos valores categ√≥ricos solo aparecen en un √∫nico ejemplo, es el caso de la caracter√≠stica ‚Äúmarca‚Äù\n",
    "donde ‚Äúmercury‚Äù solamente aparece una vez y eso puede dar problemas a los encoders. Igual ocurre\n",
    "con otras caracter√≠sticas. Una soluci√≥n es ocuparse de estos casos en la fase de ingenier√≠a de\n",
    "caracter√≠sticas (es lo que haremos en este ejemplo). Pero se puede perder conocimiento.\n",
    "Otra soluci√≥n es a√±adir m√°s datos (posibilidad que no tenemos) o encargarse de garantizar que\n",
    "siempre est√©n estos ejemplos en el conjunto de entrenamiento. Este procedimiento necesita una\n",
    "implementaci√≥n m√°s compleja. Podemos crearnos un dataset temporal y a√±adirlo a los datos de train\n",
    "los ejemplos con valores escasos de manera artificial. Para ello creamos una condici√≥n para cada\n",
    "caracter√≠stica que tenga estos valores escasos y podemos generar sus datos. Completa las dos\n",
    "condiciones que faltan del siguiente c√≥digo:"
   ],
   "id": "3e733c916024fa86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cond_1 = \"sistema in ['mfi', 'spfi']\"\n",
    "cond_2 = \"cilindros in [2,3]\"\n",
    "cond_3 = \"cilindros in [4,5]\"\n",
    "unir_a_train = autos.query(f\"{cond_1} | {cond_2} | {cond_3}\")\n",
    "print(\"Datos qe no sabemos perder:\", unir_a_train)"
   ],
   "id": "5f72bb193cac86eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ESTUDIAR ESTAD√çSTICAS DE PAREJAS DE VARIABLES<br><br>\n",
    "En primer lugar vamos a estudiar si todas las caracter√≠sticas num√©ricas influyen de manera lineal en el\n",
    "target. Podemos hacerlo de manera visual creando scatterplot() de cada una con el target"
   ],
   "id": "a9adfcccc37bf579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#====== An√°lisis bivariable\n",
    "# generar listadp de variables n√∫mericas excepto target\n",
    "cols_numericas = (autos.drop(columns=[\"precio\"].select_dtypes(include=[np.number])).columns.tolist())\n",
    "n_cols_numericas = len(cols_numericas)\n",
    "import seaborn as sns\n",
    "import math\n",
    "fig, ejes = plt.subplots(math.ceil(n_cols_numericas / 5), 5, figsize=(10, 5))\n",
    "ejes = ejes.flatten()\n",
    "for i, col in enumerate(cols_numericas):\n",
    "    sns.regplot(data=autos, x=col, y=\"precio\", ax=ejes[i])\n",
    "    ejes[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "14dc525bfcd4f70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Estudiar la correlaci√≥n entre variables<br><br>\n",
    "La correlaci√≥n lineal entre las variables predictoras que sean num√©ricas. Visualmente se puede ver en\n",
    "las graficas como las anteriores pero realizadas entre cada par de predictoras o podemos hacer el\n",
    "estudio de manera num√©rica creando una matriz de correlaciones. Vamos a usar esta segunda\n",
    "aproximaci√≥n, pero antes:<br><br>\n",
    "‚Ä¢ La variable \"tama√±o_motor\" tiene una relaci√≥n lineal positiva con el target.<br><br>\n",
    "‚Ä¢ Pero \"symboling\" no tiene relaci√≥n lineal y por tanto el valor de la correlaci√≥n no es v√°lido.<br><br>\n",
    "Como estamos interesados en la cantidad de correlaci√≥n y no en si esta es positiva o negativa, lo que\n",
    "vamos a hacer es generar la matriz de correlaciones y transformarla en sus valores absolutos para\n",
    "visualizar mejor los posibles casos a contemplar. Primero ponemos la caracter√≠stica target la primer\n",
    "apara que la primera fila y la primera columna se vean claramente. Deber√≠a aparecer lo m√°s oscura\n",
    "posible. Si es inferior a 0.1 (color claro cercano al blanco) es candidata a eliminarla como predictora.<br><br>\n",
    "Adem√°s, si encontramos correlaciones importantes con otras predictoras es que hay columnas con\n",
    "colinealidad y eso tampoco es bueno, podemos eliminar la que tenga menor correlaci√≥n con target."
   ],
   "id": "d25494b571f4a869"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib.colors import Colormap as cm\n",
    "cols = autos.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('precio')))\n",
    "autos = autos.reindex(columns=cols)\n",
    "auto_matriz_correlaciones = autos.corr(numeric_only=True)\n",
    "fig, ejes = plt.subplots(figsize=(12, 10))\n",
    "absoluta = auto_matriz_correlaciones.abs()\n",
    "sns.heatmap(absoluta, annot=True, annot_kws={'size':6}, fmt=\".2f\", cmap=\"Greens\")"
   ],
   "id": "1438f4689cabc759"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Es el caso de las caracter√≠sticas longitud, anchura, peso y tama√±o del motor. Todas ellas tienen una alta\n",
    "correlaci√≥n. Ser√≠an candidatas a examinarlas en profundidad para eliminar algunas. <br><br>\n",
    "A las caracter√≠sticas que tienen valores categ√≥ricos no podemos calcularles el coeficiente de\n",
    "correlaci√≥n pero s√≠ podemos hacerles otros test estad√≠sticos como ANOVA (An√°lisis de la Varianza). Pero\n",
    "vamos simplemente a dibujar sus boxplot"
   ],
   "id": "18a1c3eadb2427ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Gr√°ficos boxplot de categoricas con el target\n",
    "n_categoricas = len(cols_categoricas)\n",
    "fig, ejes = plt.subplots(n_categoricas // 4 + 1, 4, figsize=(12, 7))\n",
    "ejes = ejes.flatten()\n",
    "for i, col in enumerate(cols_categoricas):\n",
    "    sns.boxplot(data=autos, x=\"precio\", y=col, ax=ejes[i])\n",
    "    ejes[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7f79f49f9f5014f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hay variables categ√≥ricas que permiten distinguir entre grupos de valores de la variable objetivo, por\n",
    "ejemplo la variable \"lugar_de_motor\" los dos boxplots tienen diferencias significativas para cada valor,\n",
    "por lo que es una variable que s√≠ aporta informaci√≥n al modelo. <br><br>\n",
    "La variable \"puertas\" no permite distinguir claramente grupos de valores del target, por lo que no es\n",
    "una variable que aporte informaci√≥n al modelo. Visualmente puedes identificar que la distribuci√≥n de\n",
    "los boxplots son similares para ambos valores: two √≥ four."
   ],
   "id": "83a74e7ae3ea0312"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Aunque se detectan caracter√≠sticas a eliminar, en este ejemplo, continuamos trabajando con ellas.\n",
   "id": "57b5e50805182ecf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "FASE DE INGENIER√çA DE CARACTER√çSTICAS<br><br>\n",
    "Aplicamos una imputaci√≥n simple sin hacer un an√°lisis mas profundo de los datos. Las variables\n",
    "num√©ricas se imputan con la media y las categ√≥ricas con la moda. Usamos pipelines de\n",
    "transformaci√≥n porque nos dan flexibilidad y nos van a ahorrar mucho trabajo a la larga:<br><br>\n",
    "‚Ä¢ OneHotEncoder para las variables categ√≥ricas nominales.<br><br>\n",
    "‚Ä¢ OrdinalEncoder para las variables categ√≥ricas ordinales<br><br>"
   ],
   "id": "f5b57b635af5ac6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#========== Ingenier√≠a de caracter√≠sticas\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "y = autos['precio']\n",
    "X = autos.drop('precio', axis='columns')\n",
    "# Distribuir columnas predictoras en 1 de estas categor√≠as\n",
    "cols_categoricas = [\"marca\", \"combustible\", \"inyeccion\", \"chasis\", \"traccion\", \"lugar_de_motor\", \"tipo_motor\", \"sistema\"]\n",
    "cols_categoricas_ordinales = [\"puertas\", \"cilindros\"]\n",
    "cols_todas = X.columns.tolist()\n",
    "cols_numericas = [x for x in cols_numericas if x not in cols_categoricas and x not in cols_categoricas_ordinales]\n",
    "print(\"Predictoras ordinales: \", cols_categoricas_ordinales)\n",
    "print(\"Predictoras cat√©goricas: \", cols_categoricas)\n",
    "print(\"Predictoras n√∫meros: \", cols_numericas)\n",
    "\n",
    "pipe_numericas = Pipeline(steps[('imputer', SimpleImputer(strategy=\"median\")), ('scaler', StandardScaler())])\n",
    "pipe_categoricas = Pipeline(steps=[('imputer', SimpleImputer(strategy=\"most_frequent\")), ('onehot', OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "pipe_categoricas_ordinales = Pipeline(steps=[\n",
    "    ('ordenc', OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)),\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frquent\"))\n",
    "])\n",
    "preprocesa_columnas = ColumnTransformer(transformers=[\n",
    "    ('numericas', pipe_numericas, cols_numericas),\n",
    "    ('categoricas', pipe_categoricas, cols_categoricas),\n",
    "    ('categoricas ordinales', pipe_categoricas_ordinales, cols_categoricas_ordinales),\n",
    "])"
   ],
   "id": "297617be1a5c95cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DIVIDIR EL DATASET EN TRAIN Y TEST<br><br>\n",
    "En el exterior, dejamos el 80% de los datos para entrenar y el 20% para test. No tenemos restricciones\n",
    "de balanceo, etc. Si acaso, vendr√≠a bien escalar caracter√≠sticas num√©ricas seg√∫n los algoritmos que\n",
    "usemos"
   ],
   "id": "72f16d3107ab1786"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print(\"Dimensiones de train: \",x_train.shape)\n",
    "print(\"Dimensiones de test: \",x_test.shape)"
   ],
   "id": "50f5bf60c4a6cccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "BUSCAR UN BUEN MODELO DE REGRESI√ìN<br><br>\n",
    "Si se tienen muchos modelos, realizar validaci√≥n cruzada con todos es costoso, as√≠ que se deben ir\n",
    "descartando los modelos con menos desempe√±o hasta llegar al modelo final. Una forma de hacerlo:<br><br>\n",
    "‚Ä¢ Inicialmente se dividen los datos en dos partes: Una para realizar la selecci√≥n del modelo\n",
    "(datos de selecci√≥n del modelo) y otra para realizar la prueba de desempe√±o (datos de test).\n",
    "Esta parte de los datos se debe usar solo en el final de todo el proceso.<br><br>\n",
    "‚Ä¢ Luego se hace una evaluaci√≥n de todos los modelos con la divisi√≥n anterior y se seleccionan los\n",
    "mejores (es preferible usar modelos con principios de funcionamiento diferentes entre ellos).<br><br>\n",
    "‚Ä¢ Con lo mejores modelos (la cantidad depende de los resultados) se realiza la validaci√≥n\n",
    "cruzada (detectar si hay overfitting) para obtener los que tengan mejor resultado. Se saca el\n",
    "mejor o los mejores modelos (mejor desempe√±o y poca varianza) y se realiza optimizaci√≥n de\n",
    "hiperpar√°metros. Este proceso es costoso, por eso se debe realizar con muy pocos modelos.<br><br>\n",
    "‚Ä¢ Luego se selecciona el mejor modelo (mejor desempe√±o y menor varianza) y se obtienen los\n",
    "hiperpar√°metros que dieron el mejor resultado.<br><br>\n",
    "‚Ä¢ Finalmente, se entrena el modelo seleccionado con los hiperpar√°metros encontrados con los\n",
    "datos de selecci√≥n de modelos y se hace la prueba con los datos de test.<br><br>\n",
    "Creamos una funci√≥n que se encargue de entrenar a un modelo. Esta funci√≥n dividir√° los datos que le\n",
    "pasemos de nuevo en train y test. Aunque normalmente le pasaremos un x_train e y_train externo:"
   ],
   "id": "f5f6333d59716ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures # Generar datos de potencias de los originales\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dic_resultados = {}\n",
    "\n",
    "def entrenar_modelo(modelo, procesador, X: pd.DataFrame, y: pd.Series, pct_test:float=0.2):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=pct_test, random_state=123)\n",
    "    pipe = Pipeline(steps[(\"preprocessor\", procesador), (\"model\", modelo)])"
   ],
   "id": "e7dbeb3de7645c3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hacemos otra funci√≥n para mostrar los resultados del entrenamiento:",
   "id": "84b0804bd49d79de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "685bba7f1d98b7c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora creamos y entrenamos varios modelos de regresi√≥n lineal con la funci√≥n que hemos definido y\n",
    "registramos los score de cada uno en el diccionario creado a tal fin. Para el caso del modelo de\n",
    "regresi√≥n polinomial, vamos a generar caracter√≠sticas nuevas as√≠ que para automatizar el proceso,\n",
    "creamos un pipeline que automatice este nuevo paso, de esta manera no tendremos que hacerlo\n",
    "nosotros manualmente:"
   ],
   "id": "8c0a3531a7752aa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ba907d73dfb20298"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora podemos dibujar. Pero antes una aclaraci√≥n, el modelo etiquetado como ‚ÄúDummy‚Äù es uno que\n",
    "directamente devuelve siempre un valor fijo independientemente de la entrada, as√≠ que solamente acierta si se le\n",
    "aparece la virgen en un patinete. Como eso es improbable, es muy malo, en regresi√≥n similar a uno aleatorio. De\n",
    "hecho, uno que sea peor que √©l, tiene m√©rito. Y sin embargo, uno de los que tenemos tiene tanto overfitting que\n",
    "es mucho peor, porque al menos el dummy da la mediana y no se aleja mucho en sus predicciones, tendr√° un\n",
    "score negativo (coeficiente de determinaci√≥n malo) pero no muy alto. As√≠ que cuando ejecutes el c√≥digo, el\n",
    "modelo que de un score horrible es candidato a descartarlo (aunque si antes hubi√©semos realizado limpieza de\n",
    "predictoras, selecci√≥n de caracter√≠sticas formalmente hablando, el resultado podr√≠a cambiar)."
   ],
   "id": "97bebaa8b6b66c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b4faa4fffd90799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vuelve a ejecutar el c√≥digo, y si vuelve a aparecer uno que destaque por lo malo que es, lo volvemos a nominar y\n",
    "comentamos su c√≥digo.\n"
   ],
   "id": "ad9f9c1a9107b24d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a6fb1b16d7a1946d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Al final el gr√°fico que te quedar√° parecido al de arriba, ver√°s que son todos bastante competentes (salvo el\n",
    "dummy, claro y alguno que tenga overfitting). He tachado los nombres para que tengas que averiguar tu mismo\n",
    "cuales tienen problemas si mantenemos todas las predictoras. Nos hemos quedado con 4 que parecen mejores."
   ],
   "id": "fbadb61975e68e2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "VALIDACI√ìN CRUZADA<br><br>\n",
    "Analizar la varianza de los modelos m√°s prometedores para obtener los que tengan mejor desempe√±o.\n",
    "Aunque tu debes usar los 4 que hayas considerado mejores, yo te voy a mostrar el c√≥digo de todos:\n"
   ],
   "id": "75bb67da0b7b2344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c257d81f22db24be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora vamos a definir estructuras que nos permitan realizar la CV con 10-Fold para cada modelo. Yo\n",
    "voy a usar todos y eso har√° que mis gr√°ficos no sean muy buenos al tener dos modelos muy malos."
   ],
   "id": "df595af9f29d27e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5865510aa63a287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Comparaci√≥n Estad√≠stica de Modelos<br><br>\n",
    "Ahora tras la validaci√≥n cruzada vamos a usar los resultados para comprobar si hay o no una\n",
    "diferencia significativa entre los modelos considerados. Usaremos un test ANOVA de un factor"
   ],
   "id": "8536254d471f9d7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d474b64ef6285614"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En mi caso el test no es adecuado porque una de las condiciones que asume es que las desviaciones de\n",
    "todos los datos que participan en el el test son similares (homoscedasticidad) y en mi ejecuci√≥n no se\n",
    "cumple al mantener los dos modelos que son rematadamente malos con una varianza excesiva\n",
    "respecto a los dem√°s."
   ],
   "id": "be901632f9ece56a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5cff7d65577204c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
