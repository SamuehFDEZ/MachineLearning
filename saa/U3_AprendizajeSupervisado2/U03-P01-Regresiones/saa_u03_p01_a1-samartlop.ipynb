{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "  # REGRESI√ìN CON SVM, CART, BAGGING y BOOST\n",
   "id": "1599b2fa5628ac43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üìå ACTIVIDAD 1: REPASAR ALGORITMOS SVM",
   "id": "ff03bc16a95896c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### REGRESI√ìN LINEAL CON SVM Y UNA SOLA PREDICTORA",
   "id": "580a4027487d9ddb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Crea el notebook saa_u03_p01_a5-<tus_iniciales>.ipynb donde entregar esta actividad. Utiliza pandas\n",
    "para cargar los datos del fichero \"50_startups.csv\" (puedes utilizar una copia del fichero\n",
    "u02_p03_a1_<tus_iniciales>.ipynb). Utilizaremos como predictora la columna \"I&D Spend\" (que\n",
    "significa gasto en I+D) y como target usaremos \"Profit\" (beneficios) tal y como hicimos en una pr√°ctica\n",
    "anterior."
   ],
   "id": "ad2e8bc7442bd3b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset = pd.read_csv('saa/U2_AprendizajeSupervisado/recursos/U02_P03/50_Startups.csv')\n",
    "X = dataset.iloc[:, 0].values.reshape(-1, 1)  # gasto en I+D\n",
    "y = dataset.iloc[:, -1].values.reshape(-1, 1) # beneficios"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En todos los procesos aleatorios utiliza una misma semilla, en mi caso usar√© \"123\". Divide los datos en\n",
    "train y test dejando el 70% para entrenamiento. Una vez particionados en X_train, y_train, X_test,\n",
    "y_test crea un sklearn.preprocessing.StandardScaler() y lo entrenas con X_train para normalizar\n",
    "X_train y X_test. Tambi√©n vamos a escalar los y_train e y_test con su propio objeto escalador. Este\n",
    "c√≥digo no te lo paso, debes hacerlo tu mismo. Los par√°metros de los objetos escaladores aparecen\n",
    "abajo, no tienen que coincidir porque dependen de los datos que se usen (elecci√≥n aleatoria) pero si\n",
    "deben ser razonablemente parecidos."
   ],
   "id": "71e0d60aa151c7e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Crea un objeto sklearn.Linear.LinearSVR() con hiperpar√°metro epsilon de 0.5 y tu semilla aleatoria\n",
    "y lo llamas svm. Luego lo entrenas. Para graficar la SVM escribe estas dos funciones, una que calcula los\n",
    "vectores soporte y otra que dibuja los datos train y el modelo."
   ],
   "id": "ada7ca92f28b80fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calcula_vectores_soporte(svm, X, y):\n",
    "  predicciones = svm.predict(X)\n",
    "  fuera_del_margen = (np.abs(y - predicciones) >= svm.epsilon)\n",
    "  return np.argwhere(fuera_del_margen)\n",
    "\n",
    "svm.soporte = calcula_vectores_soporte(svm, X_train, y_train)"
   ],
   "id": "15b5c1bc54d05a72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_svm_regresion(svm, X, y, intervalo_ejes):\n",
    "  x1s = np.linspace(intervalo_ejes[0], intervalo_ejes[1], 100).reshape(100, 1)\n",
    "  y_pred = svm.predict(x1s)\n",
    "  plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "  plt.plot(x1s, y_pred + svm.epsilon, \"k--\")\n",
    "  plt.plot(x1s, y_pred - svm.epsilon, \"k--\")\n",
    "  plt.scatter(X[svm.soporte_], y[svm.soporte_], s=180, facecolors='#FFAAAA')\n",
    "  plt.plot(X, y, \"bo\")\n",
    "  plt.xlabel(r\"$x_1$\", fontsize=16)\n",
    "  plt.axis(intervalo_ejes)"
   ],
   "id": "8d583b996e4de0b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Representa la SVM entrenada en un gr√°fico en el intervalo [-2.5, 2.5] de X y de Y.\n",
   "id": "ade82486e5be373d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1de0a23dc830f09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " A√±ade otro gr√°fico scatterplot() de los datos de test en color verde con la etiqueta \"test\". Haz otro\n",
    "gr√°fico scatterplot() de los datos predichos para X_test en color rojo con la etiqueta \"predicciones\"\n",
    "(deben caer sobre la l√≠nea principal de la SVM)."
   ],
   "id": "7b1b4129cca908ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Por √∫ltimo, muestra los coeficientes y el punto de corte del modelo y su score (coeficiente de\n",
    "determinaci√≥n R2) para datos de train y test."
   ],
   "id": "1749ae918763dd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ENTREGA 1:\n",
    "\n",
    "\n",
    "Muestra:\n",
    " a) C√≥digo y capturas de ejecuci√≥n.\n",
    "\n",
    " b) ¬øSi SVR(kernel=\"linear\") es equivalente a LinearSVR() qu√© diferencia hay entre ambos?\n",
    "\n",
    " c) Puesto que es necesario escalar los datos para usar las m√°quinas de vector soporte c√≥modo utilizar un , es m√°s\n",
    "pipeline que nos permita realizar las dos operaciones unificadas. Crea un pipeline con sklearn.pipeline make_pipeline() y un LinearSVR() con que integre un objeto que normalice los datos epsilon 0.5 y tu semilla aleatoria, lo entrenas y vuelves a calcular el score sobre los datos de test ¬øCoincide el score con el regresor anterior?"
   ],
   "id": "261efaf82d8b9539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6249ba866cd349f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ### CUANDO Y COMO USAR CADA POSIBLE REGRESOR SVM",
   "id": "fc409e3de057b3e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " La siguiente tabla muestra un resumen de las diferentes caracter√≠sticas de varios regresores basados en\n",
    "SVM:"
   ],
   "id": "3a9df9e928e43d7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #90EE90;\">\n",
    "      <th style=\"color: black;\">Regresor</th>\n",
    "      <thstyle=\"color: black;\">¬øCu√°ndo usarlo?</th>\n",
    "      <th style=\"color: black;\">Caracter√≠sticas clave</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>SVR</td>\n",
    "      <td>Datos no lineales, flexibilidad con $\\epsilon$</td>\n",
    "      <td>Soporta kernel, √∫til si < 10000 datos aprox.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>NuSVR</td>\n",
    "      <td>Como SVR, controla complejidad con $\\nu$</td>\n",
    "      <td>Controla cu√°ntos puntos son vectores de soporte</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>LinearSVR</td>\n",
    "      <td>Datos lineales, eficiente con muchos datos</td>\n",
    "      <td>M√°s r√°pido, sin kernels, para muchos datos</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ],
   "id": "81b0e8f2a7356bf6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Vamos a probar resultados con datos sint√©ticos que simulan una funci√≥n no lineal.",
   "id": "219b109a526d5db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9dd075e557f023"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " En primer lugar usa un modelo SVR con un kernel=\"rbf\", C=1 y gamma=0.1.\n",
    "Vamos a jugar a ajustar lo m√°ximo que podamos el modelo a los datos de entrenamiento. Debes crear una figura de 1 fila y 2 columnas, y entrenar primero el modelo original, calcular el MSE y generar el gr√°fico de como\n",
    "predice. Luego cambia los par√°metros y haz lo mismo dibujando el gr√°fico de la derecha. Recuera que:\n",
    "\n",
    " ‚Ä¢ C controla el margen (aumentas si undefitting y bajas si overfitting):\n",
    "\n",
    " ‚ó¶ C peque√±o: permite m√°s errores a cambio de m√°s suavidad en la predicci√≥n (evita\n",
    "sobreajuste).\n",
    "\n",
    " ‚ó¶ C grande: penaliza m√°s los errores ‚Üí intenta ajustarse mejor a los datos.\n",
    "\n",
    " ‚Ä¢ epsilon es la tolerancia al error, define un margen dentro del cual el error no se penaliza.\n",
    "\n",
    " ‚ó¶ Œµ peque√±o (por ejemplo 0.01) predicci√≥n m√°s precisa, intenta ajustarse a los datos.\n",
    "\n",
    " ‚ó¶ Œµ grande (1.0 o m√°s) m√°s tolerancia al error, no se ajusta tanto a los datos y reduce\n",
    "sensibilidad a datos ruidosos (generaliza mejor).\n",
    "\n",
    " ‚Ä¢ gamma define cuanta influencia tienen los puntos individuales en la forma del kernel RBF:\n",
    "\n",
    " ‚ó¶ gamma peque√±o (0.01 por ejemplo) cada punto afecta a una gran regi√≥n ‚Üí suave y\n",
    "\n",
    "general.\n",
    " ‚ó¶ gamma grande (10 o m√°s): el modelo se adapta a detalles locales."
   ],
   "id": "cbb3ee878d9f9566"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora vamos a realizar lo mismo con NuSVR, que es similar a SVR pero controlas la complejidad del\n",
    "modelo con el hiperpar√°metro un que indica el porcentaje de puntos que queremos usar como\n",
    "vectores de soporte. Comienza con kernel=\"rbf\", C=100, gamma=0.1 y nu=0.4. Entrena y visualiza su\n",
    "predicci√≥n. Luego cambia primero un, luego el resto de hiperpar√°metros hasta conseguir ajustar\n",
    "mucho los datos."
   ],
   "id": "bb61108e4b8d0290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora lo mismo con LinearSVR, que es m√°s r√°pido aunque solo sirve si los datos son ajustables\n",
    "linealmente porque no usa kernels. Comienza con C=1, epsilon=0.5. Entrena y visualiza su predicci√≥n.\n",
    "Luego cambia los hiperpar√°metros hasta conseguir ajustar mucho los datos.\n"
   ],
   "id": "8a83ebd48a7f6c62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ENTREGA 2:\n",
    "Muestra:\n",
    "\n",
    " a)\n",
    " C√≥digo y capturas de ejecuci√≥n con gr√°ficos previo y posterior y c√°lculos de MSE"
   ],
   "id": "6d7195ff00a21fe2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DETECCI√ìN DE ANOMAL√çAS CON REGRESORES",
   "id": "bc29362513da2cf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Otro uso de los modelos consiste en utilizarlos para detectar outliers. En el caso de las SVM tenemos a\n",
    "OneClassSVM, que es un clasificador y tenemos un modelo lineal regresor como RANSACRegressor.\n",
    "Veamos un ejemplo de uso:"
   ],
   "id": "9477fde19924d412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generar datos de regresi√≥n con ruido\n",
    "np.random.seed(675)                   # cambia por tu semilla!!\n",
    "X = np.random.rand(100, 1) * 10        # Caracter√≠stica entre 0 y 10\n",
    "y = 2.5 * X.flatten() + np.random.randn(100) * 2  # Linea distorsionada con ruido de amplitud 2\n",
    "\n",
    "# Introducir outliers en los datos\n",
    "outliers = np.random.randint(0, 100, 10) # Seleccionamos 10 √≠ndices aleatorios\n",
    "y[outliers] += np.random.randint(20, 50, size=10) # Aumentamos el valor de esos puntos"
   ],
   "id": "abca5171b50254e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " Ahora entrenamos un modelo lineal usando todos los datos (incluidos outliers), e imprimimos su\n",
    "configuraci√≥n y generamos un gr√°fico de datos, outliers y predicciones que hace."
   ],
   "id": "b2eeb2a1cd71942f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar datos de regresi√≥n con ruido\n",
    "np.random.seed(449)                   # cambia por tu semilla!!\n",
    "X = np.random.rand(100, 1) * 10        # Caracter√≠stica entre 0 y 10\n",
    "y = 2.5 * X.flatten() + np.random.randn(100) * 2  # Linea distorsionada con ruido de amplitud 2\n",
    "\n",
    "# Introducir outliers en los datos\n",
    "outliers_indices = np.random.randint(0, 100, 10) # Seleccionamos 10 √≠ndices aleatorios\n",
    "y[outliers_indices] += np.random.randint(20, 50, size=10) # Aumentamos el valor de esos puntos\n",
    "\n",
    "# Ajustar modelo de regresi√≥n lineal (con los outliers)\n",
    "lr_with_outliers = LinearRegression()\n",
    "lr_with_outliers.fit(X, y)\n",
    "y_pred_lr_outliers = lr_with_outliers.predict(X)\n",
    "print(\"Regresi√≥n Lineal con outliers:\")\n",
    "print(f\"Recta= {lr_with_outliers.intercept_:.2f} + ({lr_with_outliers.coef_[0]:.2f})X\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X, y, color=\"gray\", alpha=0.6, label=\"Datos\")\n",
    "plt.scatter(X[outliers_indices], y[outliers_indices], color=\"red\", label=\"Outliers\", edgecolor=\"black\", s=100)\n",
    "plt.plot(X, y_pred_lr_outliers, color=\"blue\", label=\"Regresi√≥n Lineal\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Regresi√≥n Lineal con outliers\")\n",
    "\n",
    "# Detectar outliers usando OneClassSVM\n",
    "scaler = StandardScaler()\n",
    "X_escalada = scaler.fit_transform(X)\n",
    "\n",
    "oc_svm = OneClassSVM(nu=0.1)  # nu controla la proporci√≥n de outliers esperados\n",
    "oc_svm.fit(X_escalada)\n",
    "mascara = oc_svm.predict(X_escalada)\n",
    "\n",
    "# Filtrar los datos sin outliers\n",
    "X_sin_outliers = X[mascara == 1]\n",
    "y_sin_outliers = y[mascara == 1]\n",
    "\n",
    "# Entrenar un nuevo modelo lineal sin los outliers\n",
    "lr_sin_outliers = LinearRegression()\n",
    "lr_sin_outliers.fit(X_sin_outliers, y_sin_outliers)\n",
    "y_pred_lr_sin_outliers = lr_sin_outliers.predict(X_sin_outliers)\n",
    "\n",
    "print(\"\\nRegresi√≥n Lineal sin outliers (detectados por OneClassSVM):\")\n",
    "print(f\"Recta= {lr_sin_outliers.intercept_:.2f} + ({lr_sin_outliers.coef_[0]:.2f})X\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_sin_outliers, y_sin_outliers, color=\"green\", alpha=0.6, label=\"Datos sin Outliers\")\n",
    "plt.plot(X_sin_outliers, y_pred_lr_sin_outliers, color=\"blue\", label=\"Regresi√≥n Lineal (sin Outliers)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Regresi√≥n Lineal sin Outliers (OneClassSVM)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b088f25e332d9af3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Completa el c√≥digo de arriba para que usando OneClassSVM: detecte los outliers. Cuando entrenes el\n",
    "modelo y le pidas predicciones, clasificar√° los datos no outliers como clase 1. Por tanto puedes crear\n",
    "una m√°scara con los datos que son normales mascara=oc_svm.predict(X_escalada) y usando la\n",
    "m√°scara puedes quedarte con los datos sin outliers: X[mascara] e y[mascara] con los que entrenar de\n",
    "nuevo un modelo lineal y mostrar su configuraci√≥n y predicciones: <span style=\"color:green\">Completado</span>\n"
   ],
   "id": "12fe5dc918bc5b00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " El modelo RANSAC (Random Sample Consensus) es √∫til cuando tienes datos con outliers y queremos\n",
    "entrenar un modelo de regresi√≥n que sea robusto frente a ellos con el objeto RANSACRegressor que tiene\n",
    "estos hiperpar√°metros:\n",
    "\n",
    "‚Ä¢ estimator=LinearRegression(): Modelo de regresi√≥n a usar dentro de RANSAC.\n",
    "\n",
    " ‚Ä¢ max_trials=100: N√∫mero m√°ximo de intentos para encontrar un modelo sin outliers.\n",
    "\n",
    " ‚Ä¢ residual_threshold=10: L√≠mite para considerar un punto como at√≠pico o no"
   ],
   "id": "a77187566b799845"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " Puedes hacer algo parecido a lo que has hecho con OneClassSVM, crear una m√°scara y usarla para\n",
    "quedarte solo con los datos normales. La forma de hacerlo seria: mascara = ransac.inlier_mask_. El\n",
    "resultado:"
   ],
   "id": "56158ac5c2635c3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ENTREGA 3: Muestra:\n",
    "\n",
    " a) Capturas de ejecuci√≥n y el c√≥digo donde aparezcan f√≥rmulas del modelo y gr√°ficos de\n",
    "predicciones del modelo de regresi√≥n lineal con outliers y una vez eliminados tras detectarlos\n",
    "con OneClassSVM.\n",
    "\n",
    " b) Capturas de ejecuci√≥n y el c√≥digo donde aparezcan f√≥rmulas del modelo y gr√°ficos de\n",
    "predicciones del modelo de regresi√≥n lineal con outliers y una vez eliminados tras detectarlos\n",
    "con RANSAC."
   ],
   "id": "ade60ca3ae969092"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### UTILIZACI√ìN DE KERNELS\n",
   "id": "a0e51a85b38aca0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La siguiente tabla resumen los diferentes kernels que podemos usar en SVM. En la mayor√≠a de\n",
    "ocasiones el que mejores resultados suele dar es rbf porque tiene la capacidad de adaptarse muy bien\n",
    "a datos complejos. Y en el caso de la regresi√≥n, uno de ellos suele dar malos resultados."
   ],
   "id": "123f3761738c5a2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr style=\"background-color: #90EE90;\">\n",
    "      <th style=\"color: black;\">Kernel</th>\n",
    "      <th style=\"color: black;\">Mejor Uso</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Lineal</td>\n",
    "      <td>Cuando los datos son separables con una recta</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Polin√≥mico</td>\n",
    "      <td>Si la relaci√≥n entre variables es polin√≥mica</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>RBF (Gaussiano)</td>\n",
    "      <td>Cuando la separaci√≥n entre clases es compleja y desconocida</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Sigmoide</td>\n",
    "      <td>Para relaciones parecidas a redes neuronales</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ],
   "id": "7a676b34a79393c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " Vamos a probarlos:\n",
    "\n",
    " Copia este c√≥digo en el notebook y modifica las l√≠neas 8 y 13 para que los procesos aleatorios sean\n",
    "repetibles en caso de ser necesario de manera que la semilla que elijas dependa de tu nombre y\n",
    "apellidos (n_letras_nombre concatenar n_letras_apellido1 concatenar n_letras_apellido2).\n"
   ],
   "id": "2a3ac42a353250c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generar datos artificiales (con ruido)\n",
    "np.random.seed(449)                   # Semilla 449 porque Jose = 4 Rosa=4 Rodriguez=9\n",
    "X = np.sort(10 * np.random.rand(200, 1), axis=0) # Entrada (200 ejemplos)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0]) # Salida con ruido\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=449)\n",
    "\n",
    "# Calcular rango de los datos para calcular la magnitud porcentual del error\n",
    "rango = y.max() - y.min()\n",
    "\n",
    "# Graficar resultados\n",
    "plt.scatter(X_test, y_test, color=\"red\", label=\"Datos de test\")\n",
    "plt.legend()\n",
    "plt.axhline(0, color='black', linewidth=1.3, linestyle='--') # Eje horizontal (y=0)\n",
    "plt.axvline(0, color='black', linewidth=1.3, linestyle='--') # Eje vertical"
   ],
   "id": "2294c8d1eca30ba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Si graficamos los datos de test nos saldr√° algo parecido a esto:\n",
   "id": "d8458ab62462eb16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " Vamos a utilizar un SVR con diferentes kernel pero siempre con los hiperpar√°metros C=1 y\n",
    "epsilon=0.1 para ver como modeliza estos datos. Para cada cada uno generamos el gr√°fico scatter de\n",
    "los datos predichos (x_test, y_pred) en color azul y calculamos el MSE (con\n",
    "\"mean_squared_error(y_test, y_pred)\" y la magnitud porcentual del error comparada con los\n",
    "valores que toman los datos y. Deber√≠as obtener gr√°ficos similares a estos cuatro:\n"
   ],
   "id": "c2f8ce10f8ed4509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "41ba80ea04812865"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8713bdfe78d0af31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "152ff6a107c6a40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
